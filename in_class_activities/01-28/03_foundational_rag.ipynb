{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Foundational RAG Pipeline\n",
    "\n",
    "**Retrieval-Augmented Generation**\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the RAG pipeline and why it matters\n",
    "- Implement document chunking with different strategies\n",
    "- Create embeddings and store them in a vector database\n",
    "- Build a simple retriever to find relevant context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain==1.2.7 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (1.2.7)\n",
      "Requirement already satisfied: langchain-community in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (0.4.1)\n",
      "Requirement already satisfied: langchain-groq in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (1.1.1)\n",
      "Requirement already satisfied: langchain-huggingface in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (1.2.0)\n",
      "Requirement already satisfied: langchain-text-splitters in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (1.1.0)\n",
      "Requirement already satisfied: faiss-cpu in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (1.13.2)\n",
      "Requirement already satisfied: sentence-transformers in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (5.2.2)\n",
      "Requirement already satisfied: python-dotenv in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (1.2.1)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.2.7 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from langchain==1.2.7) (1.2.7)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.7 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from langchain==1.2.7) (1.0.7)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from langchain==1.2.7) (2.12.5)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.2.7->langchain==1.2.7) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.2.7->langchain==1.2.7) (0.6.6)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.2.7->langchain==1.2.7) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.2.7->langchain==1.2.7) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.2.7->langchain==1.2.7) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.2.7->langchain==1.2.7) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.2.7->langchain==1.2.7) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.7->langchain==1.2.7) (3.0.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.7->langchain==1.2.7) (4.0.0)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.7->langchain==1.2.7) (1.0.7)\n",
      "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.7->langchain==1.2.7) (0.3.3)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.7->langchain==1.2.7) (3.6.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.7->langchain==1.2.7) (1.12.2)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.7->langchain==1.2.7) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.7->langchain==1.2.7) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->langchain==1.2.7) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->langchain==1.2.7) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->langchain==1.2.7) (0.25.0)\n",
      "Requirement already satisfied: anyio in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.7->langchain==1.2.7) (4.12.1)\n",
      "Requirement already satisfied: certifi in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.7->langchain==1.2.7) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.7->langchain==1.2.7) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.7->langchain==1.2.7) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.7->langchain==1.2.7) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain==1.2.7) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain==1.2.7) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain==1.2.7) (0.4.2)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from langchain-community) (1.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from langchain-community) (2.0.46)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from langchain-community) (3.13.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from langchain-community) (2.12.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from langchain-community) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from langchain-community) (2.4.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->langchain==1.2.7) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.7->langchain==1.2.7) (2.6.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: groq<1.0.0,>=0.30.0 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from langchain-groq) (0.37.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (1.9.0)\n",
      "Requirement already satisfied: sniffio in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (1.3.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.33.4 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from langchain-huggingface) (0.36.0)\n",
      "Requirement already satisfied: tokenizers<1.0.0,>=0.19.1 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from langchain-huggingface) (0.22.2)\n",
      "Requirement already satisfied: filelock in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2026.1.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (1.2.0)\n",
      "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from sentence-transformers) (4.57.6)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from sentence-transformers) (2.10.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from sentence-transformers) (1.8.0)\n",
      "Requirement already satisfied: scipy in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from sentence-transformers) (1.17.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2026.1.15)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: setuptools in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (80.10.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: joblib>=1.3.0 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in /Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install langchain==1.2.7 langchain-community langchain-groq langchain-huggingface langchain-text-splitters faiss-cpu sentence-transformers python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up Groq API key\n",
    "if not os.getenv('GROQ_API_KEY'):\n",
    "    os.environ['GROQ_API_KEY'] = input('Enter your Groq API key: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What is RAG?\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** solves two key problems with LLMs:\n",
    "\n",
    "1. **Knowledge**: LLMs only know what they were trained on\n",
    "2. **Hallucination**: LLMs can make up facts\n",
    "\n",
    "**Solution**: Before generating, retrieve relevant information from a knowledge base and include it in the prompt.\n",
    "\n",
    "### The RAG Pipeline\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                     INDEXING (one-time)                         ‚îÇ\n",
    "‚îÇ        Document ‚Üí Chunk ‚Üí Embed ‚Üí Store in Vector DB            ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                              ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                     RETRIEVAL (per query)                       ‚îÇ\n",
    "‚îÇ     Query ‚Üí Embed ‚Üí Search Vector DB ‚Üí Get Relevant Chunks      ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                              ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                        GENERATION                               ‚îÇ\n",
    "‚îÇ       Query + Retrieved Context ‚Üí LLM ‚Üí Answer                  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Document Loading\n",
    "\n",
    "First, let's load our sample document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cameron/Documents/ITCS_5010_Gen_AI_TA_Spring_2026/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 document(s)\n",
      "Document length: 91946 characters\n",
      "\n",
      "First 500 characters:\n",
      "College of\n",
      "Computing and Informatics\n",
      "2022-2023 UNC CHARLOTTE UNDERGRADUATE CATALOG College of Computing and Informatics | 165\n",
      "College of\n",
      "Computing and Informatics\n",
      "cci.charlotte.edu\n",
      "The University of North Carolina at Charlotte's College of Computing and Informatics (CCI) is part of a dynamic and exciting educational and research\n",
      "institution that combines the knowledge and expertise of multidisciplinary faculty, industry professionals, and students. The CCI was formed in 2000 as the\n",
      "College of In\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Load the CCI undergraduate catalog document\n",
    "loader = TextLoader(\"data/CCI_2022-2023-Undergraduate-Catalog.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Check what we loaded\n",
    "print(f\"Loaded {len(documents)} document(s)\")\n",
    "print(f\"Document length: {len(documents[0].page_content)} characters\")\n",
    "print(f\"\\nFirst 500 characters:\")\n",
    "print(documents[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chunking\n",
    "\n",
    "Documents are often too long to fit in an LLM's context window, and we only need relevant parts anyway. **Chunking** splits documents into smaller pieces.\n",
    "\n",
    "### Key Parameters\n",
    "- **chunk_size**: Maximum characters per chunk\n",
    "- **chunk_overlap**: Characters shared between consecutive chunks (prevents cutting off context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 205 chunks from the document\n",
      "\n",
      "--- Chunk 1 ---\n",
      "College of\n",
      "Computing and Informatics\n",
      "2022-2023 UNC CHARLOTTE UNDERGRADUATE CATALOG College of Computing and Informatics | 165\n",
      "College of\n",
      "Computing and Informatics\n",
      "cci.charlotte.edu\n",
      "The University of North Carolina at Charlotte's College of Computing and Informatics (CCI) is part of a dynamic and exciting educational and research\n",
      "institution that combines the knowledge and expertise of multidisciplinary faculty, industry professionals, and students. The CCI was formed in 2000 as the\n",
      "\n",
      "--- Chunk 10 ---\n",
      "‚Ä¢ Software Systems\n",
      "Undergraduate Certificates\n",
      "‚Ä¢ Game Design and Development\n",
      "Honors Program\n",
      "The Computing and Informatics Honors Program (CCI Honors) is a research-based experience designed to provide mentoring to high-achieving students to\n",
      "better prepare them for post-graduate success. CCI Honors students must complete a capstone research project under the supervision of a faculty\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Create a text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,        # Maximum characters per chunk\n",
    "    chunk_overlap=50,      # Overlap between chunks\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Try to split at these boundaries first\n",
    ")\n",
    "\n",
    "# Split the documents\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Created {len(chunks)} chunks from the document\")\n",
    "print(f\"\\n--- Chunk 1 ---\")\n",
    "print(chunks[0].page_content)\n",
    "print(f\"\\n--- Chunk 10 ---\")\n",
    "print(chunks[9].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: Different Chunk Sizes\n",
    "\n",
    "Let's see how chunk size affects the number and content of chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk size 200: 616 chunks, avg length: 158 chars\n",
      "Chunk size 500: 205 chunks, avg length: 458 chars\n",
      "Chunk size 1000: 98 chunks, avg length: 947 chars\n"
     ]
    }
   ],
   "source": [
    "# Try different chunk sizes\n",
    "for size in [200, 500, 1000]:\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=size, \n",
    "        chunk_overlap=50,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Try to split at these boundaries first\n",
    "    )\n",
    "    \n",
    "    test_chunks = splitter.split_documents(documents)\n",
    "    avg_len = sum(len(c.page_content) for c in test_chunks) / len(test_chunks)\n",
    "\n",
    "    print(f\"Chunk size {size}: {len(test_chunks)} chunks, avg length: {avg_len:.0f} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trade-offs**:\n",
    "- **Smaller chunks**: More precise retrieval, but may lose context\n",
    "- **Larger chunks**: More context, but may include irrelevant information\n",
    "\n",
    "A common starting point is **500-1000 characters** with **10-20% overlap**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Embeddings\n",
    "\n",
    "**Embeddings** convert text into numerical vectors that capture meaning. Similar texts have similar vectors.\n",
    "\n",
    "- a) \"Machine learning is AI\"  ‚Üí  [0.2, -0.5, 0.8, ...]\n",
    "- b) \"AI and ML are related\"   ‚Üí  [0.3, -0.4, 0.7, ...]  \n",
    "- c) \"I like pizza\"            ‚Üí  [-0.8, 0.1, 0.2, ...]  \n",
    "\n",
    "### Libraries:\n",
    "**sentence-transformers**\n",
    "- Developed by HuggingFace for semantic text embeddings\n",
    "- Provides pre-trained models that can convert text into dense vector representations (embeddings)\n",
    "\n",
    "**langchain-huggingface**\n",
    "- LangChain integration package that wraps sentence-transformers\n",
    "- Provides LangChain-compatible interfaces to use HuggingFace models in LangChain workflows\n",
    "\n",
    "**all-MiniLM-L6-v2 embedding model**\n",
    "- https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model...\n",
      "Embedding model loaded!\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize embedding model (downloads on first run, ~90MB)\n",
    "print(\"Loading embedding model...\")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\",  # Fast and good quality\n",
    "    model_kwargs={'device': 'cpu'}   # Use 'cuda' if you have a GPU\n",
    ")\n",
    "print(\"Embedding model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 'Machine learning is a type of artificial intelligence.'\n",
      "Embedding dimensions: 384\n",
      "First 10 values: [0.003782775951549411, -0.026872709393501282, 0.051296573132276535, 0.027737408876419067, -0.010244319215416908, -0.028220683336257935, -0.015101945959031582, -0.016157962381839752, -0.04108556732535362, 0.015193924307823181]\n"
     ]
    }
   ],
   "source": [
    "# Let's see what embeddings look like\n",
    "test_text = \"Machine learning is a type of artificial intelligence.\"\n",
    "test_embedding = embeddings.embed_query(test_text)\n",
    "\n",
    "# We will only print the first 10 entries out of 384.\n",
    "print(f\"Text: '{test_text}'\")\n",
    "print(f\"Embedding dimensions: {len(test_embedding)}\")\n",
    "print(f\"First 10 values: {test_embedding[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Similarity is Measured: Cosine Similarity\n",
    "\n",
    "**Cosine similarity** measures the angle between two vectors, ranging from -1 to 1:\n",
    "- **1.0**: Identical meaning (0¬∞ angle)\n",
    "- **0.0**: No relationship (90¬∞ angle) \n",
    "- **-1.0**: Opposite meaning (180¬∞ angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity to 'Machine learning is a type of AI':\n",
      "  1.000 - 'Machine learning is a type of AI'\n",
      "  0.729 - 'AI and machine learning are closely related'\n",
      "  0.081 - 'I like pizza'\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate similarity - similar texts have similar embeddings\n",
    "import numpy as np\n",
    "\n",
    "texts = [\n",
    "    \"Machine learning is a type of AI\",\n",
    "    \"AI and machine learning are closely related\",\n",
    "    \"I like pizza\"\n",
    "]\n",
    "\n",
    "embs = [embeddings.embed_query(t) for t in texts]\n",
    "\n",
    "# Calculate cosine similarity between first text and others\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "print(\"Similarity to 'Machine learning is a type of AI':\")\n",
    "for i, text in enumerate(texts):\n",
    "    sim = cosine_similarity(embs[0], embs[i])\n",
    "    print(f\"  {sim:.3f} - '{text}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Vector Store (FAISS)\n",
    "\n",
    "### Vector Store\n",
    "A specialized database optimized for:\n",
    "- **Storing** high-dimensional vectors (embeddings)\n",
    "- **Indexing** vectors for fast retrieval\n",
    "- **Searching** for similar vectors using distance metrics (e.g., cosine similarity)\n",
    "\n",
    "### FAISS\n",
    "- **Free & Open Source**: Developed by Meta AI Research\n",
    "- **Runs Locally**: No API calls, no cloud costs\n",
    "- **Fast**: Optimized for billion-scale similarity searches\n",
    "\n",
    "**Alternative Vector Stores:**\n",
    "- **Pinecone**, **Weaviate**, **Qdrant**: Cloud-hosted (require API keys)\n",
    "- **Chroma**, **LanceDB**: Other local options similar to FAISS\n",
    "\n",
    "**GitHub**: https://github.com/facebookresearch/faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector store from 205 chunks...\n",
      "Vector store created!\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Create vector store from our chunks\n",
    "print(f\"Creating vector store from {len(chunks)} chunks...\")\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "print(\"Vector store created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Building a Retriever\n",
    "\n",
    "A **retriever** wraps the vector store and provides a clean interface for getting relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'What are the graduation requirements for CCI students?'\n",
      "\n",
      "Retrieved 3 relevant documents\n",
      "--- Result 1 ---\n",
      "‚Ä¢ A GPA of 3.4 in CCI courses\n",
      "Students should apply in the semester prior to the semester they plan to graduate. The CCI Honors Committee will formally approve admission.\n",
      "Course Requirements\n",
      "ITSC 4750 - Honors Thesis (3)\n",
      "Certification Requirements\n",
      "To graduate with Honors in Computing and Informatics...\n",
      "\n",
      "--- Result 2 ---\n",
      "member. Upon the successful completion of the honors program in CCI, students receive Honors commendations on their transcript and in the\n",
      "commencement program.\n",
      "Admission Requirements\n",
      "Consideration for admission to the honors program may be initiated by the student or by any faculty member in the Col...\n",
      "\n",
      "--- Result 3 ---\n",
      "College Algebra.\n",
      "‚Ä¢ Other Requirements: Transfer students must present an overall ‚Ä¢ Minor\n",
      "GPA of at least 2.5 with no grade less than C in Computer Science ‚Ä¢ Second major\n",
      "courses. For internal transfer students, participation in a Change ‚Ä¢ Honors program\n",
      "of Major Workshop offered by the CCI Advising ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a retriever from the vector store\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",  \n",
    "    search_kwargs={\"k\": 3}     # Number of results to return\n",
    ")\n",
    "\n",
    "# Use the retriever\n",
    "query = \"What are the graduation requirements for CCI students?\"\n",
    "relevant_docs = retriever.invoke(query)\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(f\"\\nRetrieved {len(relevant_docs)} relevant documents\")\n",
    "\n",
    "for i, doc in enumerate(relevant_docs, 1):\n",
    "    print(f\"--- Result {i} ---\")\n",
    "    print(doc.page_content[:300] + \"...\" if len(doc.page_content) > 300 else doc.page_content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Complete RAG Pipeline\n",
    "\n",
    "Now let's put it all together: retrieve context and generate an answer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Question: What are the graduation requirements for CCI students?\n",
      "\n",
      "üí¨ Answer: **Graduation (Honors) Requirements for CCI Students ‚Äì as stated in the provided context**\n",
      "\n",
      "1. **GPA Requirements**  \n",
      "   - Overall cumulative GPA‚ÄØ‚â•‚ÄØ3.2.  \n",
      "   - GPA in CCI‚Äëspecific courses‚ÄØ‚â•‚ÄØ3.4.\n",
      "\n",
      "2. **Course Requirement**  \n",
      "   - Completion of **ITSC‚ÄØ4750 ‚Äì Honors Thesis (3 credit hours)**.\n",
      "\n",
      "3. **Honors‚ÄëProgram Certification**  \n",
      "   - Prepare and submit a description of the proposed honors research to the CCI Honors Committee.  \n",
      "   - Obtain formal approval (or recommendation) from the committee.  \n",
      "   - Upon successful completion, receive honors commendations on the transcript and in the commencement program.\n",
      "\n",
      "4. **Additional Requirements for Transfer Students**  \n",
      "   - Overall GPA‚ÄØ‚â•‚ÄØ2.5 with **no grade lower than a C** in any Computer Science course.  \n",
      "   - Internal transfer students must complete the **Change‚Äëof‚ÄëMajor Workshop** offered by the CCI Advising Center before becoming eligible to declare the Computer Science major.  \n",
      "   - Must have earned the **undergraduate certificate** required for the CS major before declaring it.\n",
      "\n",
      "5. **Other Academic Elements Mentioned**  \n",
      "   - Students may also pursue a **minor**, a **second major**, or an **honors program** as part of their overall academic plan.  \n",
      "   - A set of elective courses can be selected by the student that do not satisfy any other listed requirement.\n",
      "\n",
      "These items together constitute the graduation (honors) requirements for students in the College of Computing and Informatics as described in the provided excerpt.\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatGroq(model=\"openai/gpt-oss-20b\", temperature=0.3)\n",
    "\n",
    "def simple_rag(question: str) -> str:\n",
    "    \"\"\"A simple RAG pipeline: retrieve context, then generate answer.\"\"\"\n",
    "    \n",
    "    # Step 1: Retrieve relevant chunks\n",
    "    relevant_docs = retriever.invoke(question)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "    \n",
    "    # Step 2: Create prompt with context\n",
    "    prompt = f\"\"\"Answer the question based ONLY on the following context. \n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Step 3: Generate answer\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    return response.content\n",
    "\n",
    "\n",
    "# Test the RAG pipeline\n",
    "question = \"What are the graduation requirements for CCI students?\"\n",
    "answer = simple_rag(question)\n",
    "\n",
    "print(f\"‚ùì Question: {question}\")\n",
    "print(f\"\\nüí¨ Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì What courses are required for computer science majors?\n",
      "üí¨ Based on the passage, a Computer‚ÄØScience major must complete three groups of coursework:\n",
      "\n",
      "1. **General‚ÄëEducation requirements** ‚Äì the specific courses are listed in the university‚Äôs General Education program (the passage does not name them).\n",
      "\n",
      "2. **Mathematical & Logical Reasoning** ‚Äì  \n",
      "   * **MATH‚ÄØ1120 ‚Äì Calculus (3‚ÄØcredits)** satisfies this requirement.\n",
      "\n",
      "3. **Concentration Technical Elective Courses** ‚Äì two credit‚Äëhour blocks drawn from upper‚Äëlevel (3000‚Äë or 4000‚Äëlevel) courses offered by the College of Computing and Informatics, **excluding any courses already listed** (such as MATH‚ÄØ1120):\n",
      "   * **12 credit‚Äëhour block:** select **four** upper‚Äëlevel electives.  \n",
      "   * **18 credit‚Äëhour block:** select **six** upper‚Äëlevel electives.\n",
      "\n",
      "In total, the major requires the general‚Äëeducation courses, MATH‚ÄØ1120, and a selection of **ten** upper‚Äëlevel (3000‚Äë/4000‚Äëlevel) electives from the College of Computing and Informatics (four for the 12‚Äëcredit component and six for the 18‚Äëcredit component).\n",
      "--------------------------------------------------\n",
      "‚ùì How many credit hours are needed to graduate?\n",
      "üí¨ Based on the information provided, students are expected to have earned **approximately‚ÄØ90 credit hours** before they begin their first graduate‚Äëlevel course. This is the total credit‚Äëhour requirement referenced for graduation.\n",
      "--------------------------------------------------\n",
      "‚ùì What degree programs are within the College of Computing and Informatics?\n",
      "üí¨ The College of Computing and Informatics is organized into three departments and one interdisciplinary school, each of which offers its own degree programs:\n",
      "\n",
      "- **Department of Bioinformatics and Genomics** ‚Äì degree programs in bioinformatics and genomics.  \n",
      "- **Department of Computer Science** ‚Äì undergraduate and graduate degrees in computer science, including an additional concentration in **Security and Privacy** and a **Ph.D. in Computing and Information Systems**.  \n",
      "- **Department of Software and Information Systems** ‚Äì degree programs in software engineering and information systems.  \n",
      "- **School of Data Science** ‚Äì interdisciplinary degree programs in data science.  \n",
      "\n",
      "Together, these departments and the school constitute the full set of degree programs offered by the College of Computing and Informatics.\n",
      "--------------------------------------------------\n",
      "‚ùì What is a recipe for chocolate cake?\n",
      "üí¨ I‚Äôm sorry, but the provided context does not contain any information about a chocolate cake recipe.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Try more questions!\n",
    "questions = [\n",
    "    \"What courses are required for computer science majors?\",\n",
    "    \"How many credit hours are needed to graduate?\",\n",
    "    \"What degree programs are within the College of Computing and Informatics?\",\n",
    "    \"What is a recipe for chocolate cake?\"  # Not in our document!\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"‚ùì {q}\")\n",
    "    print(f\"üí¨ {simple_rag(q)}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned the foundational RAG pipeline:\n",
    "\n",
    "1. **Document Loading**: Load documents from files\n",
    "2. **Chunking**: Split documents into smaller pieces with `RecursiveCharacterTextSplitter`\n",
    "3. **Embeddings**: Convert text to vectors with `HuggingFaceEmbeddings`\n",
    "4. **Vector Store**: Index and search with `FAISS`\n",
    "5. **Retriever**: Clean interface for getting relevant documents\n",
    "6. **Generation**: Combine context with query and send to LLM\n",
    "\n",
    "**Key Parameters to Tune**:\n",
    "- `chunk_size`: 500-1000 is a good starting point\n",
    "- `chunk_overlap`: 10-20% of chunk size\n",
    "- `k`: Number of documents to retrieve (3-5 is common)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
